{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kolubex/.miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataloaders.mg_for_wavlm_backbone import dastaset_wavlm_backbone\n",
    "from models.wavlm_finetuning_backbone import finetune_WavLM_backbone, featExtract_finetuned_WavLM\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.AverageMeter import AverageMeter\n",
    "from utils.train_eval_utils import set_seed, save_config, train, evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import yaml\n",
    "import utils.mg_utils as utils\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "model.masked_spec_embed torch.Size([768]) 768\n",
      "model.feature_extractor.conv_layers.0.conv.weight torch.Size([512, 1, 10]) 5120\n",
      "model.feature_extractor.conv_layers.0.layer_norm.weight torch.Size([512]) 512\n",
      "model.feature_extractor.conv_layers.0.layer_norm.bias torch.Size([512]) 512\n",
      "model.feature_extractor.conv_layers.1.conv.weight torch.Size([512, 512, 3]) 786432\n",
      "model.feature_extractor.conv_layers.2.conv.weight torch.Size([512, 512, 3]) 786432\n",
      "model.feature_extractor.conv_layers.3.conv.weight torch.Size([512, 512, 3]) 786432\n",
      "model.feature_extractor.conv_layers.4.conv.weight torch.Size([512, 512, 3]) 786432\n",
      "model.feature_extractor.conv_layers.5.conv.weight torch.Size([512, 512, 2]) 524288\n",
      "model.feature_extractor.conv_layers.6.conv.weight torch.Size([512, 512, 2]) 524288\n",
      "model.feature_projection.layer_norm.weight torch.Size([512]) 512\n",
      "model.feature_projection.layer_norm.bias torch.Size([512]) 512\n",
      "model.feature_projection.projection.weight torch.Size([768, 512]) 393216\n",
      "model.feature_projection.projection.bias torch.Size([768]) 768\n",
      "model.encoder.pos_conv_embed.conv.bias torch.Size([768]) 768\n",
      "model.encoder.pos_conv_embed.conv.weight_g torch.Size([1, 1, 128]) 128\n",
      "model.encoder.pos_conv_embed.conv.weight_v torch.Size([768, 48, 128]) 4718592\n",
      "model.encoder.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.0.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.0.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.0.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.0.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.0.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.0.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.0.attention.rel_attn_embed.weight torch.Size([320, 12]) 3840\n",
      "model.encoder.layers.0.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.0.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.0.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.0.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.0.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.0.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.0.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.1.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.1.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.1.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.1.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.1.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.1.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.1.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.1.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.1.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.1.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.1.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.1.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.1.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.2.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.2.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.2.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.2.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.2.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.2.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.2.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.2.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.2.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.2.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.2.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.2.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.2.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.3.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.3.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.3.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.3.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.3.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.3.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.3.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.3.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.3.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.3.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.3.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.3.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.3.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.4.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.4.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.4.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.4.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.4.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.4.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.4.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.4.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.4.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.4.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.4.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.4.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.4.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.5.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.5.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.5.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.5.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.5.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.5.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.5.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.5.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.5.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.5.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.5.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.5.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.5.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.6.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.6.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.6.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.6.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.6.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.6.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.6.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.6.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.6.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.6.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.6.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.6.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.6.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.7.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.7.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.7.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.7.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.7.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.7.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.7.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.7.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.7.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.7.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.7.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.7.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.7.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.8.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.8.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.8.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.8.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.8.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.8.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.8.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.8.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.8.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.8.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.8.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.8.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.8.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.9.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.9.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.9.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.9.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.9.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.9.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.9.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.9.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.9.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.9.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.9.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.9.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.9.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.10.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.10.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.10.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.10.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.10.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.10.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.10.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.10.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.10.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.10.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.10.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.10.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.10.final_layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.attention.gru_rel_pos_const torch.Size([1, 12, 1, 1]) 12\n",
      "model.encoder.layers.11.attention.k_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.11.attention.k_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.attention.v_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.11.attention.v_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.attention.q_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.11.attention.q_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.attention.out_proj.weight torch.Size([768, 768]) 589824\n",
      "model.encoder.layers.11.attention.out_proj.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.attention.gru_rel_pos_linear.weight torch.Size([8, 64]) 512\n",
      "model.encoder.layers.11.attention.gru_rel_pos_linear.bias torch.Size([8]) 8\n",
      "model.encoder.layers.11.layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.11.layer_norm.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.feed_forward.intermediate_dense.weight torch.Size([3072, 768]) 2359296\n",
      "model.encoder.layers.11.feed_forward.intermediate_dense.bias torch.Size([3072]) 3072\n",
      "model.encoder.layers.11.feed_forward.output_dense.weight torch.Size([768, 3072]) 2359296\n",
      "model.encoder.layers.11.feed_forward.output_dense.bias torch.Size([768]) 768\n",
      "model.encoder.layers.11.final_layer_norm.weight torch.Size([768]) 768\n",
      "model.encoder.layers.11.final_layer_norm.bias torch.Size([768]) 768\n",
      "head_conv1d.weight torch.Size([512, 512, 17]) 4456448\n",
      "head_conv1d.bias torch.Size([512]) 512\n",
      "head_conv1.weight torch.Size([512, 768, 1]) 393216\n",
      "head_conv1.bias torch.Size([512]) 512\n",
      "head_conv2.weight torch.Size([512, 512, 1]) 262144\n",
      "head_conv2.bias torch.Size([512]) 512\n",
      "head_conv3.weight torch.Size([512, 512, 1]) 262144\n",
      "head_conv3.bias torch.Size([512]) 512\n",
      "head_linear.weight torch.Size([25, 512]) 12800\n",
      "head_linear.bias torch.Size([25]) 25\n",
      "trainable params: 99770761 || all params: 99770761 || trainable%: 100.00\n"
     ]
    }
   ],
   "source": [
    "def create_optimizer(model):\n",
    "    \"\"\"\n",
    "        If it is wavlm finetuning, sets different lr for different layers,\n",
    "        else returns Adam with same lr.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            optimizer (torch.optim.Adam): Adam optimizer with lr\n",
    "    \"\"\"\n",
    "    if(config[\"wavlm_finetuning\"]) and True:\n",
    "        # Parameters of pretrained WavLM model\n",
    "        # param_groups1 = [{'params': model.model.encoder.layers[8:12].parameters(), 'lr': config[\"wavlm_finetune\"][\"lr1\"]}]\n",
    "        \n",
    "        # Parameters of all other layers\n",
    "        # print(model.named_parameters())\n",
    "        # print them in a list\n",
    "        # for name, param in model.named_parameters(recurse= False):\n",
    "        #     print(name)\n",
    "            \n",
    "        print(\"=\"*40)\n",
    "        param_groups2 = [{'params': [p for n, p in model.named_parameters(recurse=True) if \"model\" not in n], 'lr': config[\"wavlm_finetune\"][\"lr2\"]}]\n",
    "        \n",
    "        # Combine the parameter groups\n",
    "        param_groups = param_groups2\n",
    "        optimizer = optim.AdamW(param_groups,weight_decay=0.01)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"\n",
    "    Loads the config file and overrides the config file with the command line arguments.\n",
    "    \"\"\"\n",
    "    base_conf = OmegaConf.load(\"config_finetune_audio.yaml\")\n",
    "    overrides = OmegaConf.from_cli()\n",
    "    updated_conf = OmegaConf.merge(base_conf, overrides)\n",
    "    return OmegaConf.to_container(updated_conf)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            # print(_, param.shape)\n",
    "            # print all the trainable parameters\n",
    "            print(_, param.shape, param.numel())\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "config = get_config()\n",
    "config[\"wavlm_finetuning\"] = True\n",
    "(config[\"wavlm_finetune\"][\"top_k\"])= 25\n",
    "model = finetune_WavLM_backbone(config[\"wavlm_finetune\"], config[\"hugging_face_cache_path\"]).train()\n",
    "optimizer = create_optimizer(model)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.encoder.layers.9.attention.k_proj.lora_A.default.weight torch.Size([32, 768]) 24576\n",
      "base_model.model.model.encoder.layers.9.attention.k_proj.lora_B.default.weight torch.Size([768, 32]) 24576\n",
      "base_model.model.model.encoder.layers.10.attention.k_proj.lora_A.default.weight torch.Size([32, 768]) 24576\n",
      "base_model.model.model.encoder.layers.10.attention.k_proj.lora_B.default.weight torch.Size([768, 32]) 24576\n",
      "base_model.model.model.encoder.layers.11.attention.k_proj.lora_A.default.weight torch.Size([32, 768]) 24576\n",
      "base_model.model.model.encoder.layers.11.attention.k_proj.lora_B.default.weight torch.Size([768, 32]) 24576\n",
      "base_model.model.head_conv1d.modules_to_save.default.weight torch.Size([512, 512, 17]) 4456448\n",
      "base_model.model.head_conv1d.modules_to_save.default.bias torch.Size([512]) 512\n",
      "base_model.model.head_conv1.modules_to_save.default.weight torch.Size([512, 768, 1]) 393216\n",
      "base_model.model.head_conv1.modules_to_save.default.bias torch.Size([512]) 512\n",
      "base_model.model.head_conv2.modules_to_save.default.weight torch.Size([512, 512, 1]) 262144\n",
      "base_model.model.head_conv2.modules_to_save.default.bias torch.Size([512]) 512\n",
      "base_model.model.head_conv3.modules_to_save.default.weight torch.Size([512, 512, 1]) 262144\n",
      "base_model.model.head_conv3.modules_to_save.default.bias torch.Size([512]) 512\n",
      "base_model.model.head_linear.modules_to_save.default.weight torch.Size([25, 512]) 12800\n",
      "base_model.model.head_linear.modules_to_save.default.bias torch.Size([25]) 25\n",
      "trainable params: 5536281 || all params: 105307042 || trainable%: 5.26\n"
     ]
    }
   ],
   "source": [
    "wavlm_config = config[\"wavlm_finetune\"]\n",
    "n = wavlm_config[\"num_wavlm_layers\"]\n",
    "lora_config = LoraConfig(\n",
    "    r=wavlm_config[\"lora\"][\"r\"],\n",
    "    lora_alpha=wavlm_config[\"lora\"][\"alpha\"],\n",
    "    target_modules=[\n",
    "        module.replace('*', str(11 - i))\n",
    "        for module in wavlm_config[\"lora\"][\"target_modules\"].split(\"@\")\n",
    "        for i in range(n)\n",
    "    ],            \n",
    "    bias=\"none\",\n",
    "    lora_dropout=wavlm_config[\"lora\"][\"dropout\"],\n",
    "    modules_to_save=wavlm_config[\"lora\"][\"modules_to_save\"],\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnode063                       Tue Oct 31 14:47:16 2023  525.125.06\n",
    "[0] NVIDIA GeForce RTX 2080 Ti | 82°C,  94 % |  9355 / 11264 MB | kolubex(7532M) kolubex(606M) kolubex(606M) kolubex(606M)\n",
    "[1] NVIDIA GeForce RTX 2080 Ti | 76°C, 100 % |  7535 / 11264 MB | kolubex(7532M)\n",
    "[2] NVIDIA GeForce RTX 2080 Ti | 65°C, 100 % |  7535 / 11264 MB | kolubex(7532M)\n",
    "[3] NVIDIA GeForce RTX 2080 Ti | 70°C, 100 % |  7535 / 11264 MB | kolubex(7532M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnode063                       Tue Oct 31 14:49:45 2023  525.125.06\n",
    "[0] NVIDIA GeForce RTX 2080 Ti | 73°C,  94 % |  7899 / 11264 MB | kolubex(6076M) kolubex(606M) kolubex(606M) kolubex(606M)\n",
    "[1] NVIDIA GeForce RTX 2080 Ti | 70°C, 100 % |  6079 / 11264 MB | kolubex(6076M)\n",
    "[2] NVIDIA GeForce RTX 2080 Ti | 63°C, 100 % |  5975 / 11264 MB | kolubex(5972M)\n",
    "[3] NVIDIA GeForce RTX 2080 Ti | 66°C, 100 % |  5975 / 11264 MB | kolubex(5972M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = featExtract_finetuned_WavLM(\"/ssd_scratch/cvit/kolubex/checkpoints/total/WavLM_finetuned_backbone_t25_scene.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featExtract_finetuned_WavLM(\n",
      "  (model): PeftModel(\n",
      "    (base_model): LoraModel(\n",
      "      (model): finetune_WavLM_backbone(\n",
      "        (model): WavLMModel(\n",
      "          (feature_extractor): WavLMFeatureEncoder(\n",
      "            (conv_layers): ModuleList(\n",
      "              (0): WavLMGroupNormConvLayer(\n",
      "                (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
      "                (activation): GELUActivation()\n",
      "                (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
      "              )\n",
      "              (1-4): 4 x WavLMNoLayerNormConvLayer(\n",
      "                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (5-6): 2 x WavLMNoLayerNormConvLayer(\n",
      "                (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (feature_projection): WavLMFeatureProjection(\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (projection): Linear(in_features=512, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): WavLMEncoder(\n",
      "            (pos_conv_embed): WavLMPositionalConvEmbedding(\n",
      "              (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
      "              (padding): WavLMSamePadLayer()\n",
      "              (activation): GELUActivation()\n",
      "            )\n",
      "            (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layers): ModuleList(\n",
      "              (0): WavLMEncoderLayer(\n",
      "                (attention): WavLMAttention(\n",
      "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (gru_rel_pos_linear): ModulesToSaveWrapper(\n",
      "                    (original_module): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    (modules_to_save): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                  (rel_attn_embed): Embedding(320, 12)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (feed_forward): WavLMFeedForward(\n",
      "                  (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                  (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (1-6): 6 x WavLMEncoderLayer(\n",
      "                (attention): WavLMAttention(\n",
      "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (gru_rel_pos_linear): ModulesToSaveWrapper(\n",
      "                    (original_module): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    (modules_to_save): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (feed_forward): WavLMFeedForward(\n",
      "                  (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                  (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (7-10): 4 x WavLMEncoderLayer(\n",
      "                (attention): WavLMAttention(\n",
      "                  (k_proj): Linear(\n",
      "                    in_features=768, out_features=768, bias=True\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (gru_rel_pos_linear): ModulesToSaveWrapper(\n",
      "                    (original_module): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    (modules_to_save): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (feed_forward): WavLMFeedForward(\n",
      "                  (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                  (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "              (11): WavLMEncoderLayer(\n",
      "                (attention): WavLMAttention(\n",
      "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (gru_rel_pos_linear): ModulesToSaveWrapper(\n",
      "                    (original_module): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    (modules_to_save): ModuleDict(\n",
      "                      (default): Linear(in_features=64, out_features=8, bias=True)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (feed_forward): WavLMFeedForward(\n",
      "                  (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                  (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (conv1d): ModulesToSaveWrapper(\n",
      "          (original_module): Conv1d(768, 512, kernel_size=(17,), stride=(17,), padding=(1,))\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Conv1d(768, 512, kernel_size=(17,), stride=(17,), padding=(1,))\n",
      "          )\n",
      "        )\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (linear): ModulesToSaveWrapper(\n",
      "          (original_module): Linear(in_features=512, out_features=25, bias=True)\n",
      "          (modules_to_save): ModuleDict(\n",
      "            (default): Linear(in_features=512, out_features=25, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
